import json
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# ä½¿ç”¨ä½ å·²æœ‰çš„ BGE-M3 æ¨¡å‹
model = SentenceTransformer('BAAI/bge-m3')

# å‡çº§åçš„è¯„ä¼°é›†ï¼ˆç”¨è¯­ä¹‰ç›¸ä¼¼åº¦è€Œéç²¾ç¡®åŒ¹é…ï¼‰
EVALUATION_SET = [
    {
        "question": "ä»€ä¹ˆæ˜¯å¼ºåˆ¶éš”ç¦»æˆ’æ¯’ï¼Ÿ",
        "reference_answer": """
å¼ºåˆ¶éš”ç¦»æˆ’æ¯’æ˜¯å¯¹å¸æ¯’æˆç˜¾äººå‘˜åœ¨ç‰¹å®šåœºæ‰€è¿›è¡Œå¼ºåˆ¶æ€§æˆ’æ¯’æ²»ç–—çš„æªæ–½ã€‚
ç”±å…¬å®‰æœºå…³ä½œå‡ºå†³å®šï¼Œé€‚ç”¨äºæ‹’ç»ç¤¾åŒºæˆ’æ¯’ã€åœ¨ç¤¾åŒºæˆ’æ¯’æœŸé—´å¸æ¯’ã€
æˆ–å¸æ¯’æˆç˜¾ä¸¥é‡éš¾ä»¥é€šè¿‡ç¤¾åŒºæˆ’æ¯’æˆ’é™¤çš„äººå‘˜ã€‚
ä¸æ»¡16å‘¨å²æœªæˆå¹´äººã€å­•å¦‡ã€å“ºä¹³æœŸå¦‡å¥³ä¸é€‚ç”¨ã€‚
        """.strip(),
        "key_aspects": [
            "å…¬å®‰æœºå…³å†³å®šçš„å¼ºåˆ¶æªæ–½",
            "åœ¨ç‰¹å®šåœºæ‰€è¿›è¡Œæˆ’æ¯’æ²»ç–—",
            "é€‚ç”¨æ¡ä»¶ï¼šæ‹’ç»ç¤¾åŒºæˆ’æ¯’æˆ–å¸æ¯’ä¸¥é‡",
            "ç‰¹æ®Šäººç¾¤ä¸é€‚ç”¨ï¼šæœªæˆå¹´äººã€å­•å¦‡ç­‰"
        ]
    },
    {
        "question": "ç¤¾åŒºæˆ’æ¯’çš„é€‚ç”¨æ¡ä»¶æ˜¯ä»€ä¹ˆï¼Ÿ",
        "reference_answer": """
ç¤¾åŒºæˆ’æ¯’é€‚ç”¨äºè¢«å…¬å®‰æœºå…³åœ¨ä½œå‡ºè¡Œæ”¿å¤„ç½šçš„åŒæ—¶è´£ä»¤æ¥å—ç¤¾åŒºæˆ’æ¯’çš„äººå‘˜ï¼Œ
ä»¥åŠå¼ºåˆ¶éš”ç¦»æˆ’æ¯’äººå‘˜å› æ‚£ä¸¥é‡ç–¾ç—…ã€å¥åº·çŠ¶å†µä¸å†é€‚å®œå¼ºåˆ¶éš”ç¦»æˆ’æ¯’çš„ï¼Œ
å…¬å®‰æœºå…³å¯ä»¥å˜æ›´æˆ’æ¯’æªæ–½è´£ä»¤å…¶æ¥å—ç¤¾åŒºæˆ’æ¯’ã€‚
        """.strip(),
        "key_aspects": [
            "è¡Œæ”¿å¤„ç½šåŒæ—¶è´£ä»¤æ¥å—",
            "å¥åº·åŸå› å˜æ›´å¼ºåˆ¶éš”ç¦»æªæ–½",
            "ç”±å…¬å®‰æœºå…³å†³å®š"
        ]
    },
    {
        "question": "æˆ’æ¯’äººå‘˜æœ‰å“ªäº›æƒåˆ©å’Œä¿éšœï¼Ÿ",
        "reference_answer": """
æˆ’æ¯’äººå‘˜åœ¨å…¥å­¦ã€å°±ä¸šã€äº«å—ç¤¾ä¼šä¿éšœç­‰æ–¹é¢ä¸å—æ­§è§†ã€‚
ä¸ªäººä¿¡æ¯åº”å½“ä¾æ³•äºˆä»¥ä¿å¯†ã€‚
å¿çº§ä»¥ä¸Šæ”¿åºœæ•™è‚²ã€æ°‘æ”¿ã€äººåŠ›èµ„æºç¤¾ä¼šä¿éšœéƒ¨é—¨åº”å½“åœ¨å…¥å­¦ã€å°±ä¸šã€
ç¤¾ä¼šä¿éšœç­‰æ–¹é¢å¯¹æˆ’æ¯’äººå‘˜ç»™äºˆå¿…è¦çš„æŒ‡å¯¼å’Œå¸®åŠ©ã€‚
æˆ’æ–­3å¹´æœªå¤å¸çš„äººå‘˜ï¼Œä¸å†å®è¡ŒåŠ¨æ€ç®¡æ§ã€‚
        """.strip(),
        "key_aspects": [
            "å…¥å­¦å°±ä¸šä¸å—æ­§è§†",
            "ä¸ªäººä¿¡æ¯ä¿å¯†",
            "æ”¿åºœéƒ¨é—¨ç»™äºˆæŒ‡å¯¼å¸®åŠ©",
            "æˆ’æ–­3å¹´åä¸å†ç®¡æ§"
        ]
    },
    {
        "question": "å¼ºåˆ¶éš”ç¦»æˆ’æ¯’çš„æœŸé™æ˜¯å¤šä¹…ï¼Ÿ",
        "reference_answer": """
å¼ºåˆ¶éš”ç¦»æˆ’æ¯’çš„æœŸé™ä¸ºäºŒå¹´ã€‚
å¯¹äºæˆ’æ¯’æƒ…å†µè‰¯å¥½çš„ï¼Œå¯ä»¥æå‰è§£é™¤ã€‚
å¯¹äºéœ€è¦å»¶é•¿çš„ï¼Œæœ€é•¿å¯ä»¥å»¶é•¿ä¸€å¹´ï¼Œå³æ€»æœŸé™æœ€é•¿ä¸ºä¸‰å¹´ã€‚
        """.strip(),
        "key_aspects": [
            "åŸºæœ¬æœŸé™äºŒå¹´",
            "å¯æå‰è§£é™¤",
            "æœ€é•¿å»¶é•¿ä¸€å¹´è‡³ä¸‰å¹´"
        ]
    },
    {
        "question": "æˆ’æ¯’åœºæ‰€åº”å½“æä¾›å“ªäº›æœåŠ¡ï¼Ÿ",
        "reference_answer": """
æˆ’æ¯’åœºæ‰€åº”å½“æä¾›æˆ’æ¯’åº·å¤æŒ‡å¯¼ã€å¿ƒç†å¹²é¢„ç­‰ä¸“ä¸šæœåŠ¡ã€‚
æä¾›æˆ’æ¯’åŒ»ç–—æœåŠ¡ã€å¿ƒç†åº·å¤ã€è¡Œä¸ºçŸ«æ­£ã€ç¤¾ä¼šåŠŸèƒ½æ¢å¤ç­‰æªæ–½ã€‚
å¼€å±•è‰¾æ»‹ç—…ç­‰ä¼ æŸ“ç—…çš„æ£€æµ‹å’Œé¢„é˜²æ•™è‚²ã€‚
æä¾›å¿…è¦çš„å·¥ä½œæ¡ä»¶å’Œä¿éšœã€‚
        """.strip(),
        "key_aspects": [
            "æˆ’æ¯’åº·å¤æŒ‡å¯¼",
            "å¿ƒç†å¹²é¢„å’Œå¿ƒç†åº·å¤",
            "è¡Œä¸ºçŸ«æ­£å’Œç¤¾ä¼šåŠŸèƒ½æ¢å¤",
            "åŒ»ç–—æœåŠ¡å’Œä¼ æŸ“ç—…æ£€æµ‹",
            "èŒä¸šæŠ€èƒ½åŸ¹è®­"
        ]
    }
]

def semantic_evaluate(answer, reference, key_aspects):
    """åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦çš„è¯„ä¼°"""
    
    # 1. æ•´ä½“ç›¸ä¼¼åº¦ï¼ˆ40%æƒé‡ï¼‰
    answer_emb = model.encode([answer])[0]
    ref_emb = model.encode([reference])[0]
    overall_sim = cosine_similarity([answer_emb], [ref_emb])[0][0]
    
    # 2. å…³é”®æ–¹é¢è¦†ç›–åº¦ï¼ˆ60%æƒé‡ï¼‰
    aspect_embs = model.encode(key_aspects)
    aspect_sims = cosine_similarity([answer_emb], aspect_embs)[0]
    
    # æ¯ä¸ªå…³é”®æ–¹é¢çš„å¾—åˆ†ï¼ˆç›¸ä¼¼åº¦>0.5ç®—è¦†ç›–ï¼‰
    aspect_scores = [1 if sim > 0.5 else sim for sim in aspect_sims]
    aspect_coverage = np.mean(aspect_scores)
    
    # ç»¼åˆå¾—åˆ†
    final_score = 0.4 * overall_sim + 0.6 * aspect_coverage
    
    return {
        'score': final_score * 5,  # è½¬æ¢ä¸º5åˆ†åˆ¶
        'overall_similarity': overall_sim,
        'aspect_coverage': aspect_coverage,
        'aspect_details': list(zip(key_aspects, aspect_sims))
    }

def evaluate_from_file(results_file):
    """ä»å‹æµ‹ç»“æœæ–‡ä»¶è¯„ä¼°"""
    print("\n" + "="*80)
    print("ğŸ“Š è¯­ä¹‰è´¨é‡è¯„ä¼°ï¼ˆåŸºäº BGE-M3 Embeddingï¼‰")
    print("="*80 + "\n")
    
    with open(results_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    results = data['results']
    method_scores = {}
    
    for method, answers in results.items():
        print(f"\n{'='*80}")
        print(f"è¯„ä¼°æ–¹æ³•: {method}")
        print(f"{'='*80}\n")
        
        scores = []
        detailed_results = []
        
        for eval_q in EVALUATION_SET:
            question = eval_q['question']
            answer_obj = next((a for a in answers if a['query'] == question), None)
            
            if not answer_obj:
                print(f"âš ï¸  æœªæ‰¾åˆ°é—®é¢˜çš„ç­”æ¡ˆ: {question}")
                continue
            
            answer = answer_obj['answer']
            eval_result = semantic_evaluate(
                answer,
                eval_q['reference_answer'],
                eval_q['key_aspects']
            )
            
            scores.append(eval_result['score'])
            detailed_results.append({
                'question': question,
                'score': eval_result['score'],
                'overall_sim': eval_result['overall_similarity'],
                'aspect_cov': eval_result['aspect_coverage']
            })
            
            print(f"é—®é¢˜: {question}")
            print(f"å¾—åˆ†: {eval_result['score']:.2f}/5")
            print(f"æ•´ä½“ç›¸ä¼¼åº¦: {eval_result['overall_similarity']:.2%}")
            print(f"å…³é”®ç‚¹è¦†ç›–: {eval_result['aspect_coverage']:.2%}")
            print()
        
        avg_score = np.mean(scores) if scores else 0
        method_scores[method] = {
            'average_score': avg_score,
            'total_questions': len(scores),
            'detailed_results': detailed_results
        }
        
        print(f"ğŸ“Š {method} å¹³å‡å¾—åˆ†: {avg_score:.2f}/5 ({avg_score/5*100:.1f}%)\n")
    
    # å¯¹æ¯”åˆ†æ
    print("\n" + "="*80)
    print("ğŸ“Š è´¨é‡å¯¹æ¯”æ€»ç»“ï¼ˆè¯­ä¹‰è¯„ä¼°ï¼‰")
    print("="*80 + "\n")
    
    print(f"{'æ–¹æ³•':<25} {'å¹³å‡å¾—åˆ†':<15} {'å‡†ç¡®ç‡':<15}")
    print("-" * 60)
    
    for method, data in method_scores.items():
        accuracy = data['average_score'] / 5 * 100
        print(f"{method:<25} {data['average_score']:.2f}/5{'':<9} {accuracy:.1f}%")
    
    # è®¡ç®—æå‡
    if 'baseline' in method_scores and 'hybrid_rerank' in method_scores:
        baseline_score = method_scores['baseline']['average_score']
        hybrid_score = method_scores['hybrid_rerank']['average_score']
        improvement = (hybrid_score - baseline_score) / baseline_score * 100
        
        print(f"\nâœ¨ Hybrid+Reranker ç›¸å¯¹ Baseline è´¨é‡æå‡: {improvement:+.1f}%")
    
    # ä¿å­˜æŠ¥å‘Š
    timestamp = data['timestamp']
    filename = f"semantic_quality_report_{timestamp}.json"
    
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump({
            'evaluation_method': 'semantic_similarity',
            'model': 'BAAI/bge-m3',
            'timestamp': timestamp,
            'method_scores': method_scores
        }, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ“„ è¯­ä¹‰è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜: {filename}")
    print("="*80 + "\n")

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python semantic_quality_eval.py <å‹æµ‹ç»“æœæ–‡ä»¶>")
        sys.exit(1)
    
    results_file = sys.argv[1]
    evaluate_from_file(results_file)