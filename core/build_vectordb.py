import os
import shutil
from langchain_community.document_loaders import (
    TextLoader, 
    PyPDFLoader,
)
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.documents import Document

def load_documents_from_directory(directory):
    """åŠ è½½txtå’Œpdfæ–‡ä»¶"""
    print(f"\n[åŠ è½½æ–‡æ¡£] ç›®å½•: {directory}")
    
    documents = []
    
    for filename in sorted(os.listdir(directory)):
        filepath = os.path.join(directory, filename)
        
        try:
            if filename.endswith('.txt'):
                print(f"  ğŸ“„ åŠ è½½TXT: {filename}")
                loader = TextLoader(filepath, encoding='utf-8')
                docs = loader.load()
                documents.extend(docs)
                print(f"     âœ… æˆåŠŸ")
                
            elif filename.endswith('.pdf'):
                print(f"  ğŸ“• åŠ è½½PDF: {filename}")
                loader = PyPDFLoader(filepath)
                docs = loader.load()
                documents.extend(docs)
                print(f"     âœ… æå–äº† {len(docs)} é¡µ")
                
        except Exception as e:
            print(f"     âŒ åŠ è½½å¤±è´¥: {e}")
            continue
    
    print(f"\nâœ… æ€»å…±åŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
    return documents

def clean_text(text):
    """ç®€å•æ¸…ç†æ–‡æœ¬"""
    import re
    
    # ç§»é™¤å¤šä½™ç©ºè¡Œ
    lines = [line.strip() for line in text.split('\n') if line.strip()]
    text = '\n'.join(lines)
    
    # ç§»é™¤æ˜æ˜¾çš„é¡µç 
    text = re.sub(r'ç¬¬\s*\d+\s*é¡µ', '', text)
    text = re.sub(r'Page\s+\d+', '', text)
    text = re.sub(r'-\s*\d+\s*-', '', text)
    text = re.sub(r'^\s*\d+\s*$', '', text, flags=re.MULTILINE)
    
    return text

def build_vectordb():
    print("\n" + "="*80)
    print("ğŸ”¨ æ„å»ºæˆ’æ¯’æ”¿ç­–çŸ¥è¯†åº“")
    print("="*80)
    
    # 1. åŠ è½½æ–‡æ¡£
    documents = load_documents_from_directory('data/jiedu')
    
    if not documents:
        print("âŒ é”™è¯¯ï¼šæ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–‡æ¡£ï¼")
        return
    
    # 2. æ¸…ç†æ–‡æœ¬
    print("\n[æ¸…ç†æ–‡æœ¬] ç§»é™¤é¡µç å’Œå¤šä½™ç©ºè¡Œ...")
    for doc in documents:
        doc.page_content = clean_text(doc.page_content)
    
    # 3. ç»Ÿè®¡
    total_chars = sum(len(doc.page_content) for doc in documents)
    print(f"âœ… æ–‡æ¡£æ€»å­—æ•°: {total_chars:,} å­—")
    
    # 4. åˆ†å‰²æ–‡æœ¬
    print("\n[åˆ†å‰²æ–‡æœ¬] åˆ†å—å¤„ç†...")
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=50,
        separators=["\n\n", "\n", "ã€‚", "ï¼›", "ï¼Œ", " ", ""]
    )
    splits = text_splitter.split_documents(documents)
    print(f"âœ… åˆ†å‰²æˆ {len(splits)} ä¸ªæ–‡æœ¬å—")
    
    # 5. åŠ è½½Embeddingï¼ˆä¿®å¤ï¼šä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼Œç¦ç”¨åœ¨çº¿æ£€æŸ¥ï¼‰
    print("\n[åŠ è½½æ¨¡å‹] BAAI/bge-m3ï¼ˆæœ¬åœ°æ¨¡å¼ï¼‰...")
    
    # è®¾ç½®ç¦»çº¿æ¨¡å¼
    os.environ['TRANSFORMERS_OFFLINE'] = '1'
    os.environ['HF_HUB_OFFLINE'] = '1'
    
    embeddings = HuggingFaceEmbeddings(
        model_name="BAAI/bge-m3",
        model_kwargs={
            'device': 'cuda',
            'trust_remote_code': True  # ä¿¡ä»»æœ¬åœ°ä»£ç 
        },
        encode_kwargs={'normalize_embeddings': True}
    )
    print("âœ… Embeddingæ¨¡å‹åŠ è½½å®Œæˆ")
    
    # 6. æ„å»ºå‘é‡åº“
    print("\n[æ„å»ºå‘é‡åº“] è®¡ç®—å‘é‡å¹¶å­˜å‚¨...")
    
    if os.path.exists("chroma_db_jieduo"):
        shutil.rmtree("chroma_db_jieduo")
        print("ğŸ—‘ï¸  å·²åˆ é™¤æ—§å‘é‡åº“")
    
    vectordb = Chroma.from_documents(
        documents=splits,
        embedding=embeddings,
        persist_directory="./chroma_db_jieduo",
        collection_name="jieduo_collection"
    )
    
    print("âœ… å‘é‡åº“æ„å»ºå®Œæˆï¼")
    
    # 7. æµ‹è¯•æ£€ç´¢
    print("\n" + "="*80)
    print("ğŸ§ª æµ‹è¯•æ£€ç´¢åŠŸèƒ½")
    print("="*80)
    
    test_queries = [
        "ä»€ä¹ˆæ˜¯å¼ºåˆ¶éš”ç¦»æˆ’æ¯’ï¼Ÿ",
        "ç¤¾åŒºæˆ’æ¯’çš„æ¡ä»¶æ˜¯ä»€ä¹ˆï¼Ÿ",
        "æˆ’æ¯’äººå‘˜æœ‰å“ªäº›æƒåˆ©ï¼Ÿ"
    ]
    
    for query in test_queries:
        print(f"\né—®é¢˜: {query}")
        results = vectordb.similarity_search(query, k=2)
        
        for i, doc in enumerate(results, 1):
            print(f"\n  [ç»“æœ{i}]")
            print(f"  å†…å®¹: {doc.page_content[:120]}...")
            source = doc.metadata.get('source', 'unknown')
            if '/' in source:
                source = source.split('/')[-1]
            print(f"  æ¥æº: {source}")
    
    print("\n" + "="*80)
    print("âœ… çŸ¥è¯†åº“å·²å°±ç»ªï¼")
    print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"   - å‘é‡åº“è·¯å¾„: ./chroma_db_jieduo")
    print(f"   - åŸå§‹æ–‡æ¡£æ•°: {len(documents)}")
    print(f"   - æ–‡æœ¬å—æ•°: {len(splits)}")
    print(f"   - æ€»å­—æ•°: {total_chars:,}")
    print("="*80 + "\n")

if __name__ == "__main__":
    build_vectordb()