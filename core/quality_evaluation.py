import json
import asyncio
from datetime import datetime

# è´¨é‡è¯„ä¼°é—®é¢˜é›†ï¼ˆå¸¦æ ‡å‡†ç­”æ¡ˆï¼‰
EVALUATION_SET = [
    {
        "question": "ä»€ä¹ˆæ˜¯å¼ºåˆ¶éš”ç¦»æˆ’æ¯’ï¼Ÿ",
        "key_points": [
            "å…¬å®‰æœºå…³ä½œå‡ºå†³å®š",
            "æˆ’æ¯’åœºæ‰€",
            "å¼ºåˆ¶æ€§æ•™è‚²çŸ«æ²»",
            "é€‚ç”¨æ¡ä»¶"
        ],
        "answer_type": "å®šä¹‰"
    },
    {
        "question": "ç¤¾åŒºæˆ’æ¯’çš„é€‚ç”¨æ¡ä»¶æ˜¯ä»€ä¹ˆï¼Ÿ",
        "key_points": [
            "é¦–æ¬¡å¸æ¯’",
            "æˆç˜¾ç¨‹åº¦è¾ƒè½»",
            "æœ¬äººè‡ªæ„¿",
            "å¿çº§å…¬å®‰æœºå…³å†³å®š"
        ],
        "answer_type": "æ¡ä»¶"
    },
    {
        "question": "æˆ’æ¯’äººå‘˜æœ‰å“ªäº›æƒåˆ©å’Œä¿éšœï¼Ÿ",
        "key_points": [
            "ä¸å—æ­§è§†",
            "ä¸ªäººä¿¡æ¯ä¿å¯†",
            "å…¥å­¦å°±ä¸š",
            "äº«å—ç¤¾ä¼šä¿éšœ"
        ],
        "answer_type": "æƒåˆ©"
    },
    {
        "question": "å¼ºåˆ¶éš”ç¦»æˆ’æ¯’çš„æœŸé™æ˜¯å¤šä¹…ï¼Ÿ",
        "key_points": [
            "äºŒå¹´",
            "å¯ä»¥å»¶é•¿ä¸€å¹´",
            "æœ€é•¿ä¸è¶…è¿‡ä¸‰å¹´"
        ],
        "answer_type": "äº‹å®"
    },
    {
        "question": "æˆ’æ¯’åœºæ‰€åº”å½“æä¾›å“ªäº›æœåŠ¡ï¼Ÿ",
        "key_points": [
            "æˆ’æ¯’æ²»ç–—",
            "å¿ƒç†æ²»ç–—",
            "èº«ä½“åº·å¤è®­ç»ƒ",
            "èŒä¸šæŠ€èƒ½åŸ¹è®­",
            "æ³•åˆ¶æ•™è‚²"
        ],
        "answer_type": "åˆ—ä¸¾"
    }
]

def evaluate_answer_quality(question_data, answer):
    """
    è¯„ä¼°ç­”æ¡ˆè´¨é‡
    
    è¯„åˆ†æ ‡å‡†ï¼ˆ5åˆ†åˆ¶ï¼‰ï¼š
    - 5åˆ†ï¼šåŒ…å«æ‰€æœ‰å…³é”®ç‚¹ï¼Œè¡¨è¿°æ¸…æ™°å‡†ç¡®
    - 4åˆ†ï¼šåŒ…å«å¤§éƒ¨åˆ†å…³é”®ç‚¹ï¼Œè¡¨è¿°åŸºæœ¬å‡†ç¡®
    - 3åˆ†ï¼šåŒ…å«éƒ¨åˆ†å…³é”®ç‚¹ï¼Œè¡¨è¿°æœ‰ç¼ºå¤±
    - 2åˆ†ï¼šä»…åŒ…å«å°‘é‡å…³é”®ç‚¹ï¼Œè¡¨è¿°ä¸å®Œæ•´
    - 1åˆ†ï¼šåŸºæœ¬ä¸ç›¸å…³æˆ–é”™è¯¯
    """
    key_points = question_data['key_points']
    
    # ç»Ÿè®¡åŒ…å«çš„å…³é”®ç‚¹æ•°é‡
    matched_points = 0
    for point in key_points:
        if point in answer:
            matched_points += 1
    
    # è®¡ç®—è¦†ç›–ç‡
    coverage_rate = matched_points / len(key_points)
    
    # è¯„åˆ†
    if coverage_rate >= 0.8:
        score = 5
    elif coverage_rate >= 0.6:
        score = 4
    elif coverage_rate >= 0.4:
        score = 3
    elif coverage_rate >= 0.2:
        score = 2
    else:
        score = 1
    
    return {
        'score': score,
        'matched_points': matched_points,
        'total_points': len(key_points),
        'coverage_rate': coverage_rate
    }

def evaluate_from_results_file(results_file):
    """ä»å‹æµ‹ç»“æœæ–‡ä»¶ä¸­è¯„ä¼°è´¨é‡"""
    print("\n" + "="*80)
    print("ğŸ“Š RAGç³»ç»Ÿè´¨é‡è¯„ä¼°")
    print("="*80 + "\n")
    
    # åŠ è½½å‹æµ‹ç»“æœ
    with open(results_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    results = data['results']
    
    # å¯¹æ¯ç§æ–¹æ³•è¿›è¡Œè¯„ä¼°
    method_scores = {}
    
    for method, answers in results.items():
        print(f"\n{'='*80}")
        print(f"è¯„ä¼°æ–¹æ³•: {method}")
        print(f"{'='*80}\n")
        
        scores = []
        detailed_results = []
        
        for eval_q in EVALUATION_SET:
            question = eval_q['question']
            
            # æ‰¾åˆ°å¯¹åº”çš„ç­”æ¡ˆ
            answer_obj = next((a for a in answers if a['query'] == question), None)
            
            if not answer_obj:
                print(f"âš ï¸  æœªæ‰¾åˆ°é—®é¢˜çš„ç­”æ¡ˆ: {question}")
                continue
            
            answer = answer_obj['answer']
            
            # è¯„ä¼°
            eval_result = evaluate_answer_quality(eval_q, answer)
            scores.append(eval_result['score'])
            
            detailed_results.append({
                'question': question,
                'score': eval_result['score'],
                'coverage': eval_result['coverage_rate'],
                'matched': eval_result['matched_points'],
                'total': eval_result['total_points']
            })
            
            print(f"é—®é¢˜: {question}")
            print(f"å¾—åˆ†: {eval_result['score']}/5")
            print(f"è¦†ç›–ç‡: {eval_result['coverage_rate']:.1%}")
            print(f"å…³é”®ç‚¹: {eval_result['matched_points']}/{eval_result['total_points']}")
            print()
        
        # è®¡ç®—å¹³å‡åˆ†
        avg_score = sum(scores) / len(scores) if scores else 0
        
        method_scores[method] = {
            'average_score': avg_score,
            'total_questions': len(scores),
            'detailed_results': detailed_results
        }
        
        print(f"ğŸ“Š {method} å¹³å‡å¾—åˆ†: {avg_score:.2f}/5 ({avg_score/5*100:.1f}%)\n")
    
    # å¯¹æ¯”åˆ†æ
    print("\n" + "="*80)
    print("ğŸ“Š è´¨é‡å¯¹æ¯”æ€»ç»“")
    print("="*80 + "\n")
    
    print(f"{'æ–¹æ³•':<25} {'å¹³å‡å¾—åˆ†':<15} {'å‡†ç¡®ç‡':<15}")
    print("-" * 60)
    
    for method, data in method_scores.items():
        accuracy = data['average_score'] / 5 * 100
        print(f"{method:<25} {data['average_score']:.2f}/5{'':<9} {accuracy:.1f}%")
    
    # è®¡ç®—æå‡
    if 'baseline' in method_scores and 'hybrid_rerank' in method_scores:
        baseline_score = method_scores['baseline']['average_score']
        hybrid_score = method_scores['hybrid_rerank']['average_score']
        improvement = (hybrid_score - baseline_score) / baseline_score * 100
        
        print(f"\nâœ¨ Hybrid+Reranker ç›¸å¯¹ Baseline å‡†ç¡®ç‡æå‡: {improvement:+.1f}%")
    
    # ä¿å­˜è¯„ä¼°æŠ¥å‘Š
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"quality_evaluation_report_{timestamp}.json"
    
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump({
            'timestamp': timestamp,
            'evaluation_set_size': len(EVALUATION_SET),
            'method_scores': method_scores
        }, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ“„ è´¨é‡è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜: {filename}")
    print("="*80 + "\n")
    
    return method_scores

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python quality_evaluation.py <å‹æµ‹ç»“æœæ–‡ä»¶>")
        print("ç¤ºä¾‹: python quality_evaluation.py jieduo_benchmark_results_20260128_103000.json")
        sys.exit(1)
    
    results_file = sys.argv[1]
    evaluate_from_results_file(results_file)