import asyncio
import time
from typing import List, Dict, Tuple
from dataclasses import dataclass

from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.documents import Document  # âœ… ä¿®å¤ï¼šä½¿ç”¨langchain_core
from vllm import AsyncLLMEngine, SamplingParams, AsyncEngineArgs
from sentence_transformers import CrossEncoder
from rank_bm25 import BM25Okapi
import jieba

@dataclass
class RetrievalResult:
    """æ£€ç´¢ç»“æœæ•°æ®ç±»"""
    docs: List[Document]
    method: str
    retrieval_time: float
    rerank_time: float = 0.0

class EnhancedRAGSystem:
    """å¢å¼ºç‰ˆRAGç³»ç»Ÿï¼šHybrid Search + Reranker"""
    
    def __init__(self, 
                 vector_db_path: str = "./chroma_db_jieduo",
                 embedding_model: str = "BAAI/bge-m3",
                 reranker_model: str = "BAAI/bge-reranker-base",
                 llm_model: str = "./models/qwen/Qwen2.5-7B-Instruct"):
        
        self.vector_db_path = vector_db_path
        self.embedding_model_name = embedding_model
        self.reranker_model_name = reranker_model
        self.llm_model = llm_model
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.embeddings = None
        self.vectordb = None
        self.reranker = None
        self.bm25 = None
        self.documents = None
        self.engine = None
        
        print("ğŸ“¦ EnhancedRAGSystem åˆå§‹åŒ–å®Œæˆ")
    
    async def initialize(self, 
                        gpu_memory_utilization: float = 0.87,
                        max_num_batched_tokens: int = 8192,
                        max_num_seqs: int = 64):
        """å¼‚æ­¥åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶"""
        
        print("\n" + "="*80)
        print("ğŸš€ å¼€å§‹åˆå§‹åŒ–å¢å¼ºç‰ˆRAGç³»ç»Ÿ")
        print("="*80)
        
        # 1. åŠ è½½Embeddingæ¨¡å‹
        print("\n[1/5] åŠ è½½Embeddingæ¨¡å‹...")
        self.embeddings = HuggingFaceEmbeddings(
            model_name=self.embedding_model_name,
            model_kwargs={'device': 'cuda'},
            encode_kwargs={'normalize_embeddings': True}
        )
        print("âœ… Embeddingæ¨¡å‹åŠ è½½å®Œæˆ")
        
        # 2. åŠ è½½å‘é‡æ•°æ®åº“
        print("\n[2/5] åŠ è½½å‘é‡æ•°æ®åº“...")
        self.vectordb = Chroma(
            persist_directory=self.vector_db_path,
            embedding_function=self.embeddings,
            collection_name="jieduo_collection"
        )
        
        # è·å–æ‰€æœ‰æ–‡æ¡£ç”¨äºBM25
        all_data = self.vectordb.get()
        self.documents = [
            Document(page_content=text, metadata=meta)
            for text, meta in zip(all_data['documents'], all_data['metadatas'])
        ]
        print(f"âœ… å‘é‡åº“åŠ è½½å®Œæˆï¼Œå…± {len(self.documents)} ç¯‡æ–‡æ¡£")
        
        # 3. åˆå§‹åŒ–BM25
        print("\n[3/5] åˆå§‹åŒ–BM25æ£€ç´¢å™¨...")
        tokenized_corpus = [list(jieba.cut(doc.page_content)) for doc in self.documents]
        self.bm25 = BM25Okapi(tokenized_corpus)
        print("âœ… BM25åˆå§‹åŒ–å®Œæˆ")
        
        # 4. åŠ è½½Rerankeræ¨¡å‹
        print("\n[4/5] åŠ è½½Rerankeræ¨¡å‹...")
        self.reranker = CrossEncoder(self.reranker_model_name, max_length=512, device='cuda')
        print("âœ… Rerankeræ¨¡å‹åŠ è½½å®Œæˆ")
        
        # 5. åˆå§‹åŒ–vLLMå¼•æ“
        print("\n[5/5] åˆå§‹åŒ–vLLMå¼•æ“...")
        engine_args = AsyncEngineArgs(
            model=self.llm_model,
            gpu_memory_utilization=gpu_memory_utilization,
            max_num_batched_tokens=max_num_batched_tokens,
            max_num_seqs=max_num_seqs,
            trust_remote_code=True,
            dtype="bfloat16"
        )
        self.engine = AsyncLLMEngine.from_engine_args(engine_args)
        print("âœ… vLLMå¼•æ“åˆå§‹åŒ–å®Œæˆ")
        
        print("\n" + "="*80)
        print("ğŸ‰ æ‰€æœ‰ç»„ä»¶åˆå§‹åŒ–å®Œæˆï¼")
        print("="*80 + "\n")
    
    def _vector_search(self, query: str, k: int = 10) -> List[Document]:
        """å‘é‡æ£€ç´¢ï¼ˆè¯­ä¹‰ç›¸ä¼¼ï¼‰"""
        return self.vectordb.similarity_search(query, k=k)
    
    def _bm25_search(self, query: str, k: int = 10) -> List[Document]:
        """BM25æ£€ç´¢ï¼ˆå…³é”®è¯åŒ¹é…ï¼‰"""
        tokenized_query = list(jieba.cut(query))
        scores = self.bm25.get_scores(tokenized_query)
        
        # è·å–top-kçš„ç´¢å¼•
        top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:k]
        
        return [self.documents[i] for i in top_indices]
    
    def _hybrid_search(self, query: str, k: int = 20) -> List[Document]:
        """æ··åˆæ£€ç´¢ï¼šå‘é‡ + BM25"""
        # å„æ£€ç´¢k/2ä¸ª
        vector_docs = self._vector_search(query, k=k//2)
        bm25_docs = self._bm25_search(query, k=k//2)
        
        # åˆå¹¶å»é‡ï¼ˆä¿æŒé¡ºåºï¼‰
        seen = set()
        merged = []
        for doc in vector_docs + bm25_docs:
            content = doc.page_content
            if content not in seen:
                seen.add(content)
                merged.append(doc)
        
        return merged[:k]
    
    def _rerank(self, query: str, docs: List[Document], top_k: int = 3) -> Tuple[List[Document], List[float]]:
        """Rerankeré‡æ’åº"""
        if not docs:
            return [], []
        
        # æ„é€ query-docå¯¹
        pairs = [[query, doc.page_content] for doc in docs]
        
        # æ‰“åˆ†
        scores = self.reranker.predict(pairs)
        
        # æ’åº
        ranked = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)[:top_k]
        
        ranked_docs = [doc for doc, score in ranked]
        ranked_scores = [score for doc, score in ranked]
        
        return ranked_docs, ranked_scores
    
    async def retrieve_baseline(self, query: str, k: int = 3) -> RetrievalResult:
        """Baselineï¼šçº¯å‘é‡æ£€ç´¢"""
        start = time.time()
        docs = self._vector_search(query, k=k)
        retrieval_time = time.time() - start
        
        return RetrievalResult(
            docs=docs,
            method="Baseline (Vector Only)",
            retrieval_time=retrieval_time
        )
    
    async def retrieve_with_rerank(self, query: str, k: int = 3) -> RetrievalResult:
        """æ–¹æ¡ˆ1ï¼šå‘é‡æ£€ç´¢ + Reranker"""
        start = time.time()
        candidates = self._vector_search(query, k=20)
        retrieval_time = time.time() - start
        
        rerank_start = time.time()
        docs, scores = self._rerank(query, candidates, top_k=k)
        rerank_time = time.time() - rerank_start
        
        return RetrievalResult(
            docs=docs,
            method="Vector + Reranker",
            retrieval_time=retrieval_time,
            rerank_time=rerank_time
        )
    
    async def retrieve_hybrid_rerank(self, query: str, k: int = 3) -> RetrievalResult:
        """æ–¹æ¡ˆ2ï¼šæ··åˆæ£€ç´¢ + Rerankerï¼ˆæœ€ä¼˜ï¼‰"""
        start = time.time()
        candidates = self._hybrid_search(query, k=20)
        retrieval_time = time.time() - start
        
        rerank_start = time.time()
        docs, scores = self._rerank(query, candidates, top_k=k)
        rerank_time = time.time() - rerank_start
        
        return RetrievalResult(
            docs=docs,
            method="Hybrid + Reranker",
            retrieval_time=retrieval_time,
            rerank_time=rerank_time
        )
    
    async def query(self, 
                   question: str, 
                   method: str = "hybrid_rerank",
                   max_tokens: int = 128) -> Dict:
        """
        å®Œæ•´é—®ç­”æµç¨‹
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            method: æ£€ç´¢æ–¹æ³• ["baseline", "rerank", "hybrid_rerank"]
            max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
        """
        start_time = time.time()
        
        # 1. æ£€ç´¢
        if method == "baseline":
            retrieval_result = await self.retrieve_baseline(question)
        elif method == "rerank":
            retrieval_result = await self.retrieve_with_rerank(question)
        elif method == "hybrid_rerank":
            retrieval_result = await self.retrieve_hybrid_rerank(question)
        else:
            raise ValueError(f"Unknown method: {method}")
        
        # 2. æ„é€ prompt
        context = "\n\n".join([
            f"[æ–‡æ¡£{i+1}] {doc.page_content}" 
            for i, doc in enumerate(retrieval_result.docs)
        ])
        
#         prompt = f"""è¯·åŸºäºä»¥ä¸‹å‚è€ƒæ–‡æ¡£å›ç­”é—®é¢˜ã€‚

# å‚è€ƒæ–‡æ¡£ï¼š
# {context}

# é—®é¢˜ï¼š{question}

# å›ç­”ï¼š"""
        prompt = f"""<|im_start|>system
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æˆ’æ¯’æ”¿ç­–å’¨è¯¢åŠ©æ‰‹ã€‚è¯·åŸºäºæä¾›çš„å‚è€ƒæ–‡æ¡£ï¼Œå‡†ç¡®ã€ç®€æ´åœ°å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

ã€å›ç­”è¦æ±‚ã€‘
1. ç›´æ¥å›ç­”é—®é¢˜ï¼Œä¸è¦è§£é‡Šæ€è€ƒè¿‡ç¨‹
2. ä»…æ ¹æ®å‚è€ƒæ–‡æ¡£å›ç­”ï¼Œä¸è¦ç¼–é€ ä¿¡æ¯
3. ä¿æŒç®€æ´ï¼Œæ§åˆ¶åœ¨150-250å­—
4. å¦‚æœéœ€è¦åˆ—ä¸¾ï¼Œä½¿ç”¨æ•°å­—åˆ—è¡¨

ã€å‚è€ƒæ–‡æ¡£ã€‘
{context}

ã€é—®é¢˜ã€‘
{question}

ã€å›ç­”ã€‘<|im_end|>
<|im_start|>assistant
"""
        
        # 3. ç”Ÿæˆå›ç­”
        sampling_params = SamplingParams(
            temperature=0.7,
            top_p=0.9,
            max_tokens=max_tokens,
            stop_token_ids=[151643, 151645],  # Qwen2.5 çš„ EOS token
            stop=["</s>", "<|im_end|>", "\n\nå‚è€ƒæ¥æºï¼š", "å‚è€ƒæ–‡çŒ®ï¼š", "\n\né—®é¢˜ï¼š"],
            # skip_special_tokens=False,  # ä¿ç•™ç‰¹æ®Štokenæ£€æµ‹
        )
        
        generation_start = time.time()
        request_id = f"req_{int(time.time() * 1000)}"
        
        results_generator = self.engine.generate(prompt, sampling_params, request_id)
        
        first_token_time = None
        full_answer = ""
        
        async for result in results_generator:
            if result.finished:
                full_answer = result.outputs[0].text
            else:
                if first_token_time is None:
                    first_token_time = time.time()
        
        generation_time = time.time() - generation_start
        total_time = time.time() - start_time
        
        if first_token_time is None:
            ttft = 0.01
        else:
            ttft = first_token_time - generation_start
        
        return {
            "question": question,
            "answer": full_answer,
            "retrieval_method": retrieval_result.method,
            "retrieval_time": retrieval_result.retrieval_time,
            "rerank_time": retrieval_result.rerank_time,
            "generation_time": generation_time,
            "total_time": total_time,
            "ttft": ttft,
            "retrieved_docs": len(retrieval_result.docs)
        }

# æµ‹è¯•å‡½æ•°
async def test_enhanced_rag():
    """å¿«é€Ÿæµ‹è¯•"""
    print("\n" + "="*80)
    print("ğŸ§ª å¢å¼ºç‰ˆRAGç³»ç»Ÿå¿«é€Ÿæµ‹è¯•")
    print("="*80)
    
    # åˆå§‹åŒ–
    rag = EnhancedRAGSystem()
    await rag.initialize()
    
    # æµ‹è¯•é—®é¢˜
    test_queries = [
        "ä»€ä¹ˆæ˜¯æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯ï¼Ÿ",
        "vLLMçš„æ ¸å¿ƒä¼˜åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ",
        "å¦‚ä½•ä¼˜åŒ–å¤§æ¨¡å‹æ¨ç†æ€§èƒ½ï¼Ÿ"
    ]
    
    # æµ‹è¯•ä¸‰ç§æ–¹æ³•
    methods = ["baseline", "rerank", "hybrid_rerank"]
    
    for query in test_queries[:1]:  # åªæµ‹è¯•ç¬¬ä¸€ä¸ªé—®é¢˜
        print(f"\n{'='*80}")
        print(f"é—®é¢˜: {query}")
        print(f"{'='*80}")
        
        for method in methods:
            result = await rag.query(query, method=method, max_tokens=64)
            
            print(f"\nã€{result['retrieval_method']}ã€‘")
            print(f"  æ£€ç´¢è€—æ—¶: {result['retrieval_time']:.3f}ç§’")
            if result['rerank_time'] > 0:
                print(f"  é‡æ’è€—æ—¶: {result['rerank_time']:.3f}ç§’")
            print(f"  ç”Ÿæˆè€—æ—¶: {result['generation_time']:.3f}ç§’")
            print(f"  æ€»è€—æ—¶: {result['total_time']:.3f}ç§’")
            print(f"  TTFT: {result['ttft']:.3f}ç§’")
            print(f"  å›ç­”: {result['answer'][:100]}...")
    
    print("\nâœ… æµ‹è¯•å®Œæˆï¼\n")

if __name__ == "__main__":
    asyncio.run(test_enhanced_rag())