# RAG项目性能基线（Baseline）

## 测试环境
- **日期**: 2026-01-25 11:38
- **模型**: Qwen2.5-7B-Instruct (bf16)
- **硬件**: NVIDIA A6000 (48GB)
- **框架**: vLLM v0.14.1

## 阶段1：vLLM基础推理性能

### 模型加载
- 首次加载时间: **74.48秒**
  - 模型权重加载: ~25秒
  - torch.compile编译: ~14秒
  - CUDA图捕获: ~6秒
- GPU显存占用: **24.67GB** (KV Cache)
- 可用KV Cache: 461,936 tokens

### 推理性能（单query）
| Query | 输出Tokens | 推理时间 | 吞吐量 |
|-------|-----------|---------|--------|
| Query 1 | ~32 | 0.762s | 43.56 tok/s |
| Query 2 | ~230 | 5.155s | 44.46 tok/s |
| Query 3 | ~256 | 5.761s | 44.46 tok/s |

**平均吞吐量**: 44 tokens/s

### 关键观察
1. ✅ 吞吐量稳定在44 tok/s（A6000正常水平）
2. ⚠️ 长文本输出导致延迟增加（线性关系）
3. 💡 优化方向：降低max_tokens，启用批处理

---

## 下一步优化目标（任务2.2）
- [ ] 调整 `max_tokens=128` 降低单次延迟
- [ ] 启用 `max_num_batched_tokens` 提升并发
- [ ] 测试10并发下的P99延迟
- [ ] 目标：TTFT < 1.5s，P99 < 3s
