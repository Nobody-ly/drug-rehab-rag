import asyncio
import time
import statistics
import json
from datetime import datetime

# å¯¼å…¥å¼‚æ­¥RAGç³»ç»Ÿ
import sys
sys.path.append('.')

from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from vllm import AsyncLLMEngine, SamplingParams, AsyncEngineArgs

# å‹æµ‹é…ç½®ï¼ˆä¸load_test.pyä¿æŒä¸€è‡´ï¼‰
CONCURRENCY = 10  # 10å¹¶å‘
TOTAL_REQUESTS = 100  # æ€»è¯·æ±‚æ•°

# æµ‹è¯•queryåˆ—è¡¨ï¼ˆä¸load_test.pyä¿æŒä¸€è‡´ï¼‰
TEST_QUERIES = [
    "ä»€ä¹ˆæ˜¯æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯ï¼Ÿ",
    "vLLMçš„æ ¸å¿ƒä¼˜åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ",
    "Llama2æ¨¡å‹æœ‰å“ªäº›ç‰¹ç‚¹ï¼Ÿ",
    "å¦‚ä½•ä¼˜åŒ–å¤§æ¨¡å‹æ¨ç†æ€§èƒ½ï¼Ÿ",
    "PagedAttentionçš„åŸç†æ˜¯ä»€ä¹ˆï¼Ÿ",
    "RAGæŠ€æœ¯è§£å†³äº†ä»€ä¹ˆé—®é¢˜ï¼Ÿ",
    "å‘é‡æ•°æ®åº“çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ",
    "å¦‚ä½•æå‡æ¨¡å‹ååé‡ï¼Ÿ",
]

class AsyncRAGSystemBenchmark:
    """å¼‚æ­¥RAGç³»ç»Ÿï¼ˆç”¨äºå‹æµ‹ï¼‰"""
    def __init__(self):
        print("ğŸš€ åˆå§‹åŒ–å¼‚æ­¥RAGç³»ç»Ÿ...")
        
        # 1. åŠ è½½å‘é‡åº“
        print("  [1/3] åŠ è½½å‘é‡åº“...")
        self.embeddings = HuggingFaceEmbeddings(
            model_name="BAAI/bge-m3",
            model_kwargs={'device': 'cuda'},
            encode_kwargs={'normalize_embeddings': True}
        )
        
        self.vectordb = Chroma(
            persist_directory="./chroma_db",
            embedding_function=self.embeddings,
            collection_name="rag_collection"
        )
        
        # 2. åˆå§‹åŒ–å¼‚æ­¥LLMå¼•æ“
        print("  [2/3] åˆå§‹åŒ–å¼‚æ­¥vLLMå¼•æ“...")
        engine_args = AsyncEngineArgs(
            model="./models/qwen/Qwen2.5-7B-Instruct",
            gpu_memory_utilization=0.85,
            trust_remote_code=True,
            dtype="bfloat16"
        )
        self.engine = AsyncLLMEngine.from_engine_args(engine_args)
        
        print("  [3/3] ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ âœ…\n")
    
    async def query_stream(self, question: str, request_id: str, top_k: int = 3):
        """å¼‚æ­¥æµå¼æŸ¥è¯¢ï¼ˆæ”¯æŒçœŸå®TTFTæµ‹é‡ï¼‰"""
        start_time = time.time()
        
        # Step 1: å‘é‡æ£€ç´¢
        retrieval_start = time.time()
        docs = self.vectordb.similarity_search(question, k=top_k)
        retrieval_time = time.time() - retrieval_start
        
        # Step 2: æ„å»ºPrompt
        context = "\n\n".join([
            f"[æ–‡æ¡£{i+1}] {doc.page_content}" 
            for i, doc in enumerate(docs)
        ])
        
        prompt = f"""è¯·åŸºäºä»¥ä¸‹å‚è€ƒæ–‡æ¡£å›ç­”é—®é¢˜ã€‚

å‚è€ƒæ–‡æ¡£ï¼š
{context}

é—®é¢˜ï¼š{question}

å›ç­”ï¼š"""
        
        # Step 3: æµå¼ç”Ÿæˆ
        sampling_params = SamplingParams(
            temperature=0.7,
            top_p=0.9,
            max_tokens=128
        )
        
        generation_start = time.time()
        
        # æ·»åŠ è¯·æ±‚åˆ°å¼•æ“
        results_generator = self.engine.generate(
            prompt,
            sampling_params,
            request_id
        )
        
        # æµå¼æ¥æ”¶token
        first_token_time = None
        full_answer = ""
        token_count = 0
        
        async for result in results_generator:
            if result.finished:
                # æœ€ç»ˆè¾“å‡º
                full_answer = result.outputs[0].text
                token_count = len(result.outputs[0].token_ids)
            else:
                # ä¸­é—´tokenï¼ˆç¬¬ä¸€æ¬¡è¿›å…¥æ­¤åˆ†æ”¯æ—¶è®°å½•TTFTï¼‰
                if first_token_time is None:
                    first_token_time = time.time()
        
        generation_time = time.time() - generation_start
        total_time = time.time() - start_time
        
        # å¦‚æœæ²¡æœ‰æ•è·åˆ°ä¸­é—´tokenï¼Œç”¨ä¼°ç®—å€¼
        if first_token_time is None:
            ttft = generation_time / token_count if token_count > 0 else 0
        else:
            ttft = first_token_time - generation_start
        
        return {
            "request_id": request_id,
            "question": question,
            "answer": full_answer,
            "retrieval_time": retrieval_time,
            "generation_time": generation_time,
            "total_time": total_time,
            "ttft": ttft,
            "token_count": token_count,
            "throughput": token_count / generation_time if generation_time > 0 else 0,
            "success": True
        }

class AsyncLoadTester:
    def __init__(self):
        self.results = []
        self.errors = 0
        self.start_time = None
    
    async def run_single_request(self, rag_system, query_idx):
        """æ‰§è¡Œå•ä¸ªè¯·æ±‚"""
        query = TEST_QUERIES[query_idx % len(TEST_QUERIES)]
        request_id = f"req_{query_idx}_{int(time.time() * 1000)}"
        
        try:
            result = await rag_system.query_stream(query, request_id)
            return result
        except Exception as e:
            self.errors += 1
            return {
                "success": False,
                "request_id": request_id,
                "question": query,
                "error": str(e)
            }
    
    async def run_load_test(self, num_requests, concurrency):
        """æ‰§è¡Œå¼‚æ­¥å¹¶å‘å‹æµ‹"""
        print("="*80)
        print(f"ğŸ“Š å¼‚æ­¥æµå¼RAGå¹¶å‘å‹æµ‹")
        print(f"   - æ€»è¯·æ±‚æ•°: {num_requests}")
        print(f"   - å¹¶å‘æ•°: {concurrency}")
        print(f"   - æŸ¥è¯¢ç±»å‹: {len(TEST_QUERIES)}ç§")
        print("="*80 + "\n")
        
        # åˆå§‹åŒ–RAGç³»ç»Ÿï¼ˆåªåˆå§‹åŒ–ä¸€æ¬¡ï¼‰
        rag_system = AsyncRAGSystemBenchmark()
        
        self.start_time = time.time()
        
        # åˆ›å»ºæ‰€æœ‰ä»»åŠ¡
        tasks = []
        for i in range(num_requests):
            task = self.run_single_request(rag_system, i)
            tasks.append(task)
        
        # ä½¿ç”¨Semaphoreæ§åˆ¶å¹¶å‘åº¦
        semaphore = asyncio.Semaphore(concurrency)
        
        async def bounded_task(task):
            async with semaphore:
                return await task
        
        # æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
        print("â³ å¼€å§‹æ‰§è¡Œå‹æµ‹...\n")
        results = await asyncio.gather(*[bounded_task(task) for task in tasks])
        
        # æ”¶é›†ç»“æœ
        for result in results:
            if result.get("success", False):
                self.results.append(result)
        
        total_duration = time.time() - self.start_time
        
        print(f"\nâœ… å‹æµ‹å®Œæˆï¼Œè€—æ—¶: {total_duration:.2f}ç§’\n")
        
        # ç”ŸæˆæŠ¥å‘Š
        self.generate_report(total_duration)
    
    def generate_report(self, total_duration):
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        if not self.results:
            print("âŒ æ²¡æœ‰æˆåŠŸçš„è¯·æ±‚ï¼")
            return
        
        # æå–æ—¶é—´æ•°æ®
        total_times = [r["total_time"] for r in self.results]
        retrieval_times = [r["retrieval_time"] for r in self.results]
        generation_times = [r["generation_time"] for r in self.results]
        ttfts = [r["ttft"] for r in self.results]
        throughputs = [r["throughput"] for r in self.results]
        
        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        def calc_percentile(data, percentile):
            sorted_data = sorted(data)
            index = int(len(sorted_data) * percentile / 100)
            return sorted_data[min(index, len(sorted_data) - 1)]
        
        report = {
            "test_config": {
                "total_requests": len(self.results) + self.errors,
                "successful_requests": len(self.results),
                "failed_requests": self.errors,
                "concurrency": CONCURRENCY,
                "duration": f"{total_duration:.2f}s"
            },
            "throughput": {
                "qps": len(self.results) / total_duration,
                "avg_response_time": statistics.mean(total_times),
                "avg_token_throughput": statistics.mean(throughputs)
            },
            "latency": {
                "total_time": {
                    "min": min(total_times),
                    "max": max(total_times),
                    "mean": statistics.mean(total_times),
                    "median": statistics.median(total_times),
                    "p95": calc_percentile(total_times, 95),
                    "p99": calc_percentile(total_times, 99)
                },
                "ttft": {
                    "min": min(ttfts),
                    "max": max(ttfts),
                    "mean": statistics.mean(ttfts),
                    "median": statistics.median(ttfts),
                    "p95": calc_percentile(ttfts, 95),
                    "p99": calc_percentile(ttfts, 99)
                },
                "retrieval_time": {
                    "mean": statistics.mean(retrieval_times),
                    "p99": calc_percentile(retrieval_times, 99)
                },
                "generation_time": {
                    "mean": statistics.mean(generation_times),
                    "p99": calc_percentile(generation_times, 99)
                }
            }
        }
        
        # æ‰“å°æŠ¥å‘Š
        print("="*80)
        print("ğŸ“Š å¼‚æ­¥æµå¼RAGå‹æµ‹ç»“æœ")
        print("="*80 + "\n")
        
        print("ã€æµ‹è¯•é…ç½®ã€‘")
        print(f"  æ€»è¯·æ±‚æ•°: {report['test_config']['total_requests']}")
        print(f"  æˆåŠŸè¯·æ±‚: {report['test_config']['successful_requests']}")
        print(f"  å¤±è´¥è¯·æ±‚: {report['test_config']['failed_requests']}")
        print(f"  å¹¶å‘æ•°: {report['test_config']['concurrency']}")
        print(f"  æµ‹è¯•æ—¶é•¿: {report['test_config']['duration']}\n")
        
        print("ã€ååé‡æŒ‡æ ‡ã€‘")
        print(f"  QPS: {report['throughput']['qps']:.2f} è¯·æ±‚/ç§’")
        print(f"  å¹³å‡å“åº”æ—¶é—´: {report['throughput']['avg_response_time']:.3f}ç§’")
        print(f"  å¹³å‡tokenåå: {report['throughput']['avg_token_throughput']:.2f} tok/s\n")
        
        print("ã€å»¶è¿ŸæŒ‡æ ‡ - TTFTï¼ˆå…³é”®æŒ‡æ ‡ï¼‰ã€‘")
        print(f"  æœ€å°TTFT: {report['latency']['ttft']['min']:.3f}ç§’")
        print(f"  æœ€å¤§TTFT: {report['latency']['ttft']['max']:.3f}ç§’")
        print(f"  å¹³å‡TTFT: {report['latency']['ttft']['mean']:.3f}ç§’")
        print(f"  ä¸­ä½TTFT: {report['latency']['ttft']['median']:.3f}ç§’")
        print(f"  P95 TTFT: {report['latency']['ttft']['p95']:.3f}ç§’")
        print(f"  P99 TTFT: {report['latency']['ttft']['p99']:.3f}ç§’\n")
        
        print("ã€å»¶è¿ŸæŒ‡æ ‡ - ç«¯åˆ°ç«¯ã€‘")
        print(f"  æœ€å°å€¼: {report['latency']['total_time']['min']:.3f}ç§’")
        print(f"  æœ€å¤§å€¼: {report['latency']['total_time']['max']:.3f}ç§’")
        print(f"  å¹³å‡å€¼: {report['latency']['total_time']['mean']:.3f}ç§’")
        print(f"  ä¸­ä½æ•°: {report['latency']['total_time']['median']:.3f}ç§’")
        print(f"  P95: {report['latency']['total_time']['p95']:.3f}ç§’")
        print(f"  P99: {report['latency']['total_time']['p99']:.3f}ç§’\n")
        
        print("ã€å»¶è¿ŸæŒ‡æ ‡ - æ£€ç´¢ã€‘")
        print(f"  å¹³å‡æ£€ç´¢æ—¶é—´: {report['latency']['retrieval_time']['mean']:.3f}ç§’")
        print(f"  P99æ£€ç´¢æ—¶é—´: {report['latency']['retrieval_time']['p99']:.3f}ç§’\n")
        
        print("ã€å»¶è¿ŸæŒ‡æ ‡ - ç”Ÿæˆã€‘")
        print(f"  å¹³å‡ç”Ÿæˆæ—¶é—´: {report['latency']['generation_time']['mean']:.3f}ç§’")
        print(f"  P99ç”Ÿæˆæ—¶é—´: {report['latency']['generation_time']['p99']:.3f}ç§’\n")
        
        # ä¿å­˜JSONæŠ¥å‘Š
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"async_load_test_report_{timestamp}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump({
                'report': report,
                'raw_results': self.results
            }, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {filename}")
        print("="*80 + "\n")
        
        return report

async def main():
    """ä¸»å‡½æ•°"""
    tester = AsyncLoadTester()
    await tester.run_load_test(
        num_requests=TOTAL_REQUESTS,
        concurrency=CONCURRENCY
    )

if __name__ == "__main__":
    asyncio.run(main())