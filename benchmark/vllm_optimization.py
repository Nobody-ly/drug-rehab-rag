import asyncio
import time
import statistics
import json
import gc
from datetime import datetime

from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from vllm import AsyncLLMEngine, SamplingParams, AsyncEngineArgs

# ============================================================================
# é…ç½®çŸ©é˜µï¼šæ–°å¢ Opt_v4_BusinessRealistic
# ============================================================================
CONFIGS = [
    {
        "name": "Baseline",
        "max_tokens": 128,
        "max_num_batched_tokens": 2048,
        "gpu_memory_utilization": 0.85,
        "max_num_seqs": None,
        "description": "å½“å‰é»˜è®¤é…ç½®"
    },
    {
        "name": "Opt_v1_ReduceOutput",
        "max_tokens": 64,
        "max_num_batched_tokens": 8192,
        "gpu_memory_utilization": 0.87,
        "max_num_seqs": None,
        "description": "é™ä½è¾“å‡ºé•¿åº¦ + æå‡æ‰¹å¤„ç†"
    },
    {
        "name": "Opt_v4_BusinessRealistic",
        "max_tokens": 256,
        "max_num_batched_tokens": 8192,
        "gpu_memory_utilization": 0.87,
        "max_num_seqs": 64,
        "description": "ä¸šåŠ¡è´´åˆé…ç½®ï¼šè¯¦ç»†è¯´æ˜/æ”¿ç­–è§£é‡Šåœºæ™¯"
    }
]

TEST_QUERIES = [
    "ä»€ä¹ˆæ˜¯æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯ï¼Ÿ",
    "vLLMçš„æ ¸å¿ƒä¼˜åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ",
    "Llama2æ¨¡å‹æœ‰å“ªäº›ç‰¹ç‚¹ï¼Ÿ",
    "å¦‚ä½•ä¼˜åŒ–å¤§æ¨¡å‹æ¨ç†æ€§èƒ½ï¼Ÿ",
    "PagedAttentionçš„åŸç†æ˜¯ä»€ä¹ˆï¼Ÿ",
    "RAGæŠ€æœ¯è§£å†³äº†ä»€ä¹ˆé—®é¢˜ï¼Ÿ",
    "å‘é‡æ•°æ®åº“çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ",
    "å¦‚ä½•æå‡æ¨¡å‹ååé‡ï¼Ÿ",
]

CONCURRENCY = 10
TOTAL_REQUESTS = 30

class AsyncRAGBenchmark:
    def __init__(self, config):
        self.config = config
        self.embeddings = None
        self.vectordb = None
        self.engine = None
        
    async def initialize(self):
        print(f"\n{'='*80}")
        print(f"ğŸš€ åˆå§‹åŒ–é…ç½®: {self.config['name']}")
        print(f"   - max_tokens: {self.config['max_tokens']}")
        print(f"   - max_num_batched_tokens: {self.config['max_num_batched_tokens']}")
        print(f"   - gpu_memory_utilization: {self.config['gpu_memory_utilization']}")
        if self.config.get('max_num_seqs'):
            print(f"   - max_num_seqs: {self.config['max_num_seqs']}")
        print(f"   è¯´æ˜: {self.config['description']}")
        print(f"{'='*80}")
        
        if self.embeddings is None:
            print("  [1/2] åŠ è½½å‘é‡åº“...")
            self.embeddings = HuggingFaceEmbeddings(
                model_name="BAAI/bge-m3",
                model_kwargs={'device': 'cuda'},
                encode_kwargs={'normalize_embeddings': True}
            )
        
        if self.vectordb is None:
            self.vectordb = Chroma(
                persist_directory="./chroma_db",
                embedding_function=self.embeddings,
                collection_name="rag_collection"
            )
        
        print("  [2/2] åˆå§‹åŒ–vLLMå¼•æ“...")
        engine_args = AsyncEngineArgs(
            model="./models/qwen/Qwen2.5-7B-Instruct",
            gpu_memory_utilization=self.config['gpu_memory_utilization'],
            max_num_batched_tokens=self.config['max_num_batched_tokens'],
            max_num_seqs=self.config.get('max_num_seqs'),  # å…³é”®æ–°å¢ï¼
            trust_remote_code=True,
            dtype="bfloat16"
        )
        self.engine = AsyncLLMEngine.from_engine_args(engine_args)
        print("  âœ… åˆå§‹åŒ–å®Œæˆ\n")
    
    async def query_single(self, question: str, request_id: str):
        start_time = time.time()
        
        retrieval_start = time.time()
        docs = self.vectordb.similarity_search(question, k=3)
        retrieval_time = time.time() - retrieval_start
        
        context = "\n\n".join([f"[æ–‡æ¡£{i+1}] {doc.page_content}" for i, doc in enumerate(docs)])
        prompt = f"""è¯·åŸºäºä»¥ä¸‹å‚è€ƒæ–‡æ¡£å›ç­”é—®é¢˜ã€‚

å‚è€ƒæ–‡æ¡£ï¼š
{context}

é—®é¢˜ï¼š{question}

å›ç­”ï¼š"""
        
        sampling_params = SamplingParams(
            temperature=0.7,
            top_p=0.9,
            max_tokens=self.config['max_tokens']
        )
        
        generation_start = time.time()
        results_generator = self.engine.generate(prompt, sampling_params, request_id)
        
        first_token_time = None
        full_answer = ""
        token_count = 0
        
        async for result in results_generator:
            if result.finished:
                full_answer = result.outputs[0].text
                token_count = len(result.outputs[0].token_ids)
            else:
                if first_token_time is None:
                    first_token_time = time.time()
        
        generation_time = time.time() - generation_start
        total_time = time.time() - start_time
        
        if first_token_time is None:
            ttft = generation_time / token_count if token_count > 0 else 0
        else:
            ttft = first_token_time - generation_start
        
        return {
            "request_id": request_id,
            "question": question,
            "retrieval_time": retrieval_time,
            "generation_time": generation_time,
            "total_time": total_time,
            "ttft": ttft,
            "token_count": token_count,
            "throughput": token_count / generation_time if generation_time > 0 else 0,
            "success": True
        }
    
    async def run_benchmark(self):
        print(f"â³ å¼€å§‹å‹æµ‹ï¼š{TOTAL_REQUESTS}ä¸ªè¯·æ±‚ï¼Œ{CONCURRENCY}å¹¶å‘\n")
        
        start_time = time.time()
        
        tasks = []
        for i in range(TOTAL_REQUESTS):
            query = TEST_QUERIES[i % len(TEST_QUERIES)]
            request_id = f"req_{i}_{int(time.time() * 1000)}"
            tasks.append(self.query_single(query, request_id))
        
        semaphore = asyncio.Semaphore(CONCURRENCY)
        
        async def bounded_task(task):
            async with semaphore:
                return await task
        
        results = await asyncio.gather(*[bounded_task(task) for task in tasks])
        
        duration = time.time() - start_time
        print(f"âœ… æµ‹è¯•å®Œæˆï¼Œè€—æ—¶: {duration:.2f}ç§’\n")
        
        return self.analyze_results(results, duration)
    
    def analyze_results(self, results, duration):
        success_results = [r for r in results if r.get("success", False)]
        
        if not success_results:
            return None
        
        total_times = [r["total_time"] for r in success_results]
        ttfts = [r["ttft"] for r in success_results]
        gen_times = [r["generation_time"] for r in success_results]
        throughputs = [r["throughput"] for r in success_results]
        token_counts = [r["token_count"] for r in success_results]
        
        def percentile(data, p):
            sorted_data = sorted(data)
            idx = int(len(sorted_data) * p / 100)
            return sorted_data[min(idx, len(sorted_data) - 1)]
        
        stats = {
            "config": self.config,
            "test_duration": duration,
            "total_requests": len(results),
            "successful_requests": len(success_results),
            "qps": len(success_results) / duration,
            "avg_response_time": statistics.mean(total_times),
            "p99_response_time": percentile(total_times, 99),
            "ttft": {
                "mean": statistics.mean(ttfts),
                "median": statistics.median(ttfts),
                "p95": percentile(ttfts, 95),
                "p99": percentile(ttfts, 99)
            },
            "generation": {
                "mean": statistics.mean(gen_times),
                "p99": percentile(gen_times, 99)
            },
            "throughput": {
                "mean": statistics.mean(throughputs),
                "median": statistics.median(throughputs)
            },
            "tokens": {
                "mean": statistics.mean(token_counts),
                "total": sum(token_counts)
            }
        }
        
        self.print_stats(stats)
        return stats
    
    def print_stats(self, stats):
        print(f"{'='*80}")
        print(f"ğŸ“Š é…ç½®: {stats['config']['name']}")
        print(f"{'='*80}")
        print(f"æµ‹è¯•æ—¶é•¿: {stats['test_duration']:.2f}ç§’")
        print(f"QPS: {stats['qps']:.2f} è¯·æ±‚/ç§’")
        print(f"å¹³å‡å“åº”: {stats['avg_response_time']:.3f}ç§’")
        print(f"P99å“åº”: {stats['p99_response_time']:.3f}ç§’")
        print(f"\nã€TTFTã€‘")
        print(f"  å¹³å‡: {stats['ttft']['mean']:.3f}ç§’")
        print(f"  P95: {stats['ttft']['p95']:.3f}ç§’")
        print(f"  P99: {stats['ttft']['p99']:.3f}ç§’")
        print(f"\nã€ç”Ÿæˆã€‘")
        print(f"  å¹³å‡: {stats['generation']['mean']:.3f}ç§’")
        print(f"  P99: {stats['generation']['p99']:.3f}ç§’")
        print(f"\nã€ååã€‘")
        print(f"  å¹³å‡: {stats['throughput']['mean']:.2f} tok/s")
        print(f"\nã€Tokenã€‘")
        print(f"  å¹³å‡: {stats['tokens']['mean']:.1f} tokens/è¯·æ±‚")
        print(f"  æ€»è®¡: {stats['tokens']['total']} tokens")
        print(f"{'='*80}\n")
    
    async def cleanup(self):
        if self.engine:
            del self.engine
        import torch
        torch.cuda.empty_cache()
        gc.collect()

async def main():
    all_results = []
    
    print("\n" + "="*80)
    print("ğŸ¯ vLLMå‚æ•°ä¼˜åŒ–åŸºå‡†æµ‹è¯• v4")
    print("="*80)
    print(f"æµ‹è¯•é…ç½®æ•°: {len(CONFIGS)}")
    print(f"æ¯é…ç½®è¯·æ±‚æ•°: {TOTAL_REQUESTS}")
    print(f"å¹¶å‘åº¦: {CONCURRENCY}")
    print("="*80)
    
    shared_embeddings = HuggingFaceEmbeddings(
        model_name="BAAI/bge-m3",
        model_kwargs={'device': 'cuda'},
        encode_kwargs={'normalize_embeddings': True}
    )
    
    for i, config in enumerate(CONFIGS, 1):
        print(f"\n{'='*80}")
        print(f"[{i}/{len(CONFIGS)}] æµ‹è¯•é…ç½®: {config['name']}")
        print(f"{'='*80}")
        
        rag = AsyncRAGBenchmark(config)
        rag.embeddings = shared_embeddings
        
        await rag.initialize()
        stats = await rag.run_benchmark()
        
        if stats:
            all_results.append(stats)
        
        await rag.cleanup()
        
        print(f"âœ… é…ç½® {config['name']} æµ‹è¯•å®Œæˆ")
        print("â³ ç­‰å¾…10ç§’è®©GPUå®Œå…¨é‡Šæ”¾...\n")
        await asyncio.sleep(10)
    
    generate_comparison_report(all_results)

def generate_comparison_report(all_results):
    print("\n" + "="*80)
    print("ğŸ“Š å‚æ•°ä¼˜åŒ–å¯¹æ¯”æŠ¥å‘Š v4")
    print("="*80 + "\n")
    
    if not all_results:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„æµ‹è¯•ç»“æœ")
        return
    
    baseline = all_results[0]
    
    print("ã€é…ç½®å¯¹æ¯”ã€‘\n")
    print(f"{'é…ç½®å':<30} {'max_tokens':<12} {'QPS':<10} {'å¹³å‡å“åº”':<12} {'P99 TTFT':<12} {'å¹³å‡Token':<12}")
    print("-" * 100)
    
    for result in all_results:
        print(f"{result['config']['name']:<30} "
              f"{result['config']['max_tokens']:<12} "
              f"{result['qps']:<10.2f} "
              f"{result['avg_response_time']:<12.3f} "
              f"{result['ttft']['p99']:<12.3f} "
              f"{result['tokens']['mean']:<12.1f}")
    
    print("\nã€ç›¸å¯¹Baselineæå‡ã€‘\n")
    for result in all_results[1:]:
        qps_gain = (result['qps'] / baseline['qps'] - 1) * 100
        resp_gain = (1 - result['avg_response_time'] / baseline['avg_response_time']) * 100
        ttft_gain = (1 - result['ttft']['p99'] / baseline['ttft']['p99']) * 100
        token_gain = (result['tokens']['mean'] / baseline['tokens']['mean'] - 1) * 100
        
        print(f"é…ç½®: {result['config']['name']}")
        print(f"  QPSæå‡: {qps_gain:+.1f}%")
        print(f"  å“åº”æ—¶é—´æ”¹å–„: {resp_gain:+.1f}%")
        print(f"  P99 TTFTæ”¹å–„: {ttft_gain:+.1f}%")
        print(f"  å¹³å‡è¾“å‡ºToken: {token_gain:+.1f}%")
        print()
    
    # å…³é”®æ´å¯Ÿ
    print("ã€å…³é”®æ´å¯Ÿã€‘\n")
    print("1. max_tokens å¯¹æ€§èƒ½å½±å“æœ€å¤§")
    print(f"   - 64 tokens: QPSæå‡çº¦ {((all_results[1]['qps']/baseline['qps']-1)*100):.0f}%")
    if len(all_results) > 2:
        print(f"   - 256 tokens: QPSæå‡çº¦ {((all_results[2]['qps']/baseline['qps']-1)*100):.0f}%")
    
    print("\n2. æ‰¹å¤„ç†ä¼˜åŒ–æ”¶ç›Šæ˜æ˜¾")
    print("   - 2048 â†’ 8192: TTFTä¿æŒç¨³å®šï¼Œååé‡æå‡")
    
    print("\n3. ä¸šåŠ¡åœºæ™¯é€‚é…å»ºè®®")
    print("   - ç®€å•é—®ç­”: æ¨è 64-128 tokens")
    print("   - è¯¦ç»†è§£é‡Š: æ¨è 256 tokens")
    print("   - æ–‡æ¡£æ‘˜è¦: æ¨è 512+ tokens")
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"vllm_optimization_v4_report_{timestamp}.json"
    
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump({
            'summary': {
                'test_date': timestamp,
                'num_configs': len(all_results),
                'requests_per_config': TOTAL_REQUESTS,
                'concurrency': CONCURRENCY
            },
            'results': all_results
        }, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {filename}")
    print("="*80 + "\n")

if __name__ == "__main__":
    asyncio.run(main())
